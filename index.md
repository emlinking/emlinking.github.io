---
layout: default
title: Eleanor M. Lin
---
<link rel="stylesheet" href="css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="css/style.css">

<br />
<br />
<img src="images/IMG_0382.JPG" width="200"/>
 
<h3 id="name" class="p-0 m-0 mb-1">Eleanor M. Lin</h3>

B. A. Candidate, Computer Science and Linguistics  
[Department of Computer Science](https://www.cs.columbia.edu/) and [Program of Linguistics](https://slavic.columbia.edu/content/linguistics)  
Columbia University  
e.lin2@columbia.edu

I am a senior double-majoring in linguistics and computer science at Columbia University. At Columbia's [Speech Lab](http://www.cs.columbia.edu/speech/index.cgi), I currently research code-switching, under the mentorship of Professor [Julia Hirschberg](http://www.cs.columbia.edu/~julia/). Recently, I researched the use of large language models for the study of negotiation at the University of Southern California's [Affective Computing Lab](https://emotions.ict.usc.edu/), where I was advised by Professor [Jonathan Gratch](https://people.ict.usc.edu/~gratch/). Previously, working with Professor [Vicente Ordóñez-Román](https://www.cs.rice.edu/~vo9/) in the [Vision, Language, and Learning Lab](https://vislang.ai/) at Rice University, I investigated the relationship between images' visual complexity and image descriptions' linguistic complexity. Working with [Dr. Kate Moore](https://www.linkedin.com/in/kate-moore-644aab9) in the [Corter Lab](https://www.tc.columbia.edu/faculty/jec34/) at Columbia's Teachers College, I have also researched communication patterns in collaborative learning. I am primarily interested in natural language processing.

* TOC
{:toc}

## Publications
<div class="blog-post subtext p-2">
	<ul class="list-unstyled">
		<li class="media">
			<img class="mr-3 img-thumbnail" src="images/llm_nego.png" width="100" alt="">
			<div class="media-body">
			<p class="my-auto">
			<a class="blue_link" href="https://doi.org/10.1145/3565287.3617637">
				Toward a Better Understanding of the Emotional Dynamics of Negotiation with Large Language Models
			</a> 
			<span class="pub_authors d-lg-block">
				<u>Eleanor Lin</u>, James Hale, and Jonathan Gratch
			</span>
			<span class="pub_info d-inline">
				 In <i>The Twenty-fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing (MobiHoc ’23), October 23–26, 2023, Washington, DC, USA.</i> ACM, New York, NY, USA, 6 pages. 
			</span>
			[<a href="https://doi.org/10.1145/3565287.3617637">Paper</a>] 
			[<a href="files/nego_slides.pdf">Slides</a>] 
			[<a href="files/nego_poster.pdf">Poster</a>] 
			</p>
			</div>
		</li>
	</ul>
</div><!-- /.blog-post -->

## Presentations
<div class="blog-post subtext p-2">
	<ul class="list-unstyled">
		<li class="media">
			<img class="mr-3 img-thumbnail" src="images/complex_noncomplex1024_1.jpg" width="100" alt="">
			<div class="media-body">
			<p class="my-auto">
				<a class="blue_link" href="files/dreu_report.pdf">
					Text-Based Prediction of Visual Complexity: How Does What We See Influence What We Say?
				</a> 
				<span class="pub_authors d-lg-block">
					<u>Lin, E.</u>, Yang, Z., & Ordóñez, V.
				</span>
				<span class="pub_info d-inline">
					Columbia University Undergraduate Research Symposium, New York, NY, October 2022. 
				</span>
				[<a href="files/dreu_report.pdf">Report</a>] 
				[<a href="files/dreu_slides.pdf">Slides</a>] 
				[<a href="files/poster_visual_complexity_lin_2022.pdf">Poster</a>] 
				[<a href="https://github.com/uvavision/visual-complexity>Code</a>]
			</p>
			</div>
		</li>
	</ul>
</div><!-- /.blog-post -->

## Research Experience
<div class="blog-post subtext p-2">
	<ul class="list-unstyled">
		<li class="media">
			<img class="mr-3 img-thumbnail" src="images/speech_lab.png" width="100" alt="">
			<div class="media-body">
			<p class="my-auto">
			<a class="blue_link" href="http://www.cs.columbia.edu/speech/index.cgi">
				Speech Lab
			</a>, Columbia University, Department of Computer Science, New York, NY
			<span class="pub_authors d-lg-block">
				<i>Research Assistant</i>, January 2022 - Present | <i>Adviser:</i> Julia Hirschberg
			</span>
			<span class="pub_info d-inline">
				 Investigate relationship between dialog acts and code-switching, by adapting dialog act annota-
tion scheme to better suit, and annotating dialog acts on, code-switched speech. Wrote script to
fine-tune Wav2Vec2 for end-to-end dialog act classification on Switchboard Dialog Act Corpus,
using PyTorch and Hugging Face Transformers. Contributed to multimodal, multilingual empa-
thetic speech corpus by processing video, audio, text with Jupyter Notebooks, Praat, FFmpeg.
			</span>
			</p>
			</div>
		</li>
	</ul>
</div><!-- /.blog-post -->
<div class="blog-post subtext p-2">
	<ul class="list-unstyled">
		<li class="media">
			<img class="mr-3 img-thumbnail" src="images/llm_nego.png" width="100" alt="">
			<div class="media-body">
			<p class="my-auto">
			<a class="blue_link" href="https://emotions.ict.usc.edu/">
				Affective Computing Lab
			</a>, University of Southern California, Institute for Creative Technologies, Los Angeles, CA
			<span class="pub_authors d-lg-block">
				<i>Research Experiences for Undergraduates (REU) Intern</i>, May 2023 - August 2023 | <i>Adviser:</i> Jonathan Gratch
			</span>
			<span class="pub_info d-inline">
				 Developed negotiating agent for online studies of human negotiation behaviors. Engineered
large language model prompts for agent NLU/NLG with OpenAI API. Built user interface using
HTML, CSS, JavaScript, Flask. Assembled Qualtrics survey for crowdworkers to evaluate agent.
			</span>
			</p>
			</div>
		</li>
	</ul>
</div><!-- /.blog-post -->
## Teaching Experience

## Work Experience

<div style="text-align: right"> Website design inspired by <a href="https://jr4fs.github.io/">jr4fs.github.io</a>.</div>
<br />
<br />
